using DecisionMakingAlgorithms

"""
An example of how to use an exploration strategy with Q-learning with action value function approximation in simulation. 
The parameter settings are notional.

We are interesting in applying Q-learning with a linear action value approximation to the simple regulator problem with Œ≥ = 1. 
Our action value approximation is Q Œ∏ ( s, a ) = Œ∏ ‚ä§ Œ≤ ( s, a ) , 
where our basis function is Œ≤ ( s, a ) = [ s, s 2 , a, a 2 , 1 ]

An example of how to use an exploration strategy with Q-learning with action value function approximation in simulation. 
The parameter settings are notional. 
With this linear model, ‚àá Œ∏ Q Œ∏ ( s, a ) = Œ≤ ( s, a )
We can implement this as follows for problem problem :
"""

Œ≤(s, a) = [s, s^2, a, a^2, 1]
Q(Œ∏, s, a) = dot(Œ∏, Œ≤(s, a))
‚àáQ(Œ∏, s, a) = Œ≤(s, a)
Œ∏ = [0.1, 0.2, 0.3, 0.4, 0.5] # initial parameter vector
Œ± = 0.5 # learning rate
model = GradientQLearning(problem.ùíú, problem.Œ≥, Q, ‚àáQ, Œ∏, Œ±)
œµ = 0.1 # probability of random action
Œ± = 1.0 # exploration decay factor
œÄ = EpsilonGreedyExploration(œµ, Œ±)
k = 20 # number of steps to simulate
s = 0.0 # initial state
simulate(problem, model, œÄ, k, s)