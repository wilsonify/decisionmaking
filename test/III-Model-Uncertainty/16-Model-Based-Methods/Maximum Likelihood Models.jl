mutable struct MaximumLikelihoodMDP
    """
    A method for updating the transition and reward
    model for maximum likelihood reinforcement learning with discrete
    state and action spaces. We increment N[s,a,sâ€²] after observing a
    transition from s to sâ€² after taking
    action a , and we add r to Ï[s,a] .
    The model also contains an estimate of the value function U and
    a planner. This algorithm block
    also includes methods for performing backup and lookahead with respect to this model.
    """
    ğ’®::Any # state space (assumes 1:nstates)
    ğ’œ::Any # action space (assumes 1:nactions)
    N::Any # transition count N(s,a,sâ€²)
    Ï::Any # reward sum Ï(s, a)
    Î³::Any # discount
    U::Any # value function
    planner::Any
end
function lookahead(model::MaximumLikelihoodMDP, s, a)
    ğ’®, U, Î³ = model.ğ’®, model.U, model.Î³
    n = sum(model.N[s, a, :])
    if n == 0
        return 0.0
    end
    r = model.Ï[s, a] / n
    T(s, a, sâ€²) = model.N[s, a, sâ€²] / n
    return r + Î³ * sum(T(s, a, sâ€²) * U[sâ€²] for sâ€² in ğ’®)
end


function backup(model::MaximumLikelihoodMDP, U, s)
    return maximum(lookahead(model, s, a) for a in model.ğ’œ)
end
function update!(model::MaximumLikelihoodMDP, s, a, r, sâ€²)
    model.N[s, a, sâ€²] += 1
    model.Ï[s, a] += r
    update!(model.planner, model, s, a, r, sâ€²)
    return model
end

function MDP(model::MaximumLikelihoodMDP)
    """
    A method for
    converting a maximum likelihood
    model to an MDP problem.
    """
    N, Ï, ğ’®, ğ’œ, Î³ = model.N, model.Ï, model.ğ’®, model.ğ’œ, model.Î³
    T, R = similar(N), similar(Ï)
    for s in ğ’®
        for a in ğ’œ
            n = sum(N[s, a, :])
            if n == 0
                T[s, a, :] .= 0.0
                R[s, a] = 0.0
            else
                T[s, a, :] = N[s, a, :] / n
                R[s, a] = Ï[s, a] / n
            end
        end
    end
    return MDP(T, R, Î³)
end
