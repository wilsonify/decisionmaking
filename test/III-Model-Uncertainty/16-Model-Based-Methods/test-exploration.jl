# We can apply Ç«-greedy exploration to maximum likelihood model estimates constructed while interacting with the environment. 
# The code below initializes the counts, rewards, and utilities to zero. 
# It uses full updates to the value function with each step. 
# For exploration, we choose a random action with probability 0.1 without decay. 
# The last line runs a simulation (algorithm 15.9) of problem ğ’« for 100 steps.
ğ’®=[]
N = zeros(length(ğ’®), length(ğ’œ), length(ğ’®))
Ï = zeros(length(ğ’®), length(ğ’œ))
U = zeros(length(ğ’®))
planner = FullUpdate()
model = MaximumLikelihoodMDP(ğ’®, ğ’œ, N, Ï, Î³, U, planner)
Ï€ = EpsilonGreedyExploration(0.1, 1)
simulate(problem, model, Ï€, 100)

# Alternatively, we can use R-MAX with an exploration threshold of m = 3.
# We can act greedily with respect to the R-MAX model.

rmax = maximum(problem.R(s, a) for s in ğ’®, a in ğ’œ)
m = 3
model = RmaxMDP(ğ’®, ğ’œ, N, Ï, Î³, U, planner, m, rmax)
Ï€ = EpsilonGreedyExploration
