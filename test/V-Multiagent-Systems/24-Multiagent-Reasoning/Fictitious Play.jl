include("Simple Games.jl")
"""
The simulation of
a joint policy in simple game ğ’« for
k_max iterations. The joint policy Ï€
is a vector of policies that can be
individually updated through calls
to update!(Ï€i, a) .
"""
function simulate(ğ’«::SimpleGame, Ï€, k_max)
    for k = 1:k_max
        a = [Ï€i() for Ï€i in Ï€]
        for Ï€i in Ï€
            update!(Ï€i, a)
        end
    end
    return Ï€
end


mutable struct FictitiousPlay
    """
    Fictitious play is
    a simple learning algorithm for an
    agent i of a simple game ğ’« that
    maintains counts of other agent action selections over time and averages them assuming this is their
    stochastic policy. It then computes
    a best response to this policy and
    performs the corresponding utilitymaximizing action.
    (Ï€i::FictitiousPlay)() = Ï€i.Ï€i()
    (Ï€i::FictitiousPlay)(ai) = Ï€i.Ï€i(ai)
    """
    ğ’«::Any # simple game
    i::Any # agent index
    N::Any # array of action count dictionaries
    Ï€i::Any # current policy
end
function FictitiousPlay(ğ’«::SimpleGame, i)
    N = [Dict(aj => 1 for aj in ğ’«.ğ’œ[j]) for j in ğ’«.â„]
    Ï€i = SimpleGamePolicy(ai => 1.0 for ai in ğ’«.ğ’œ[i])
    return FictitiousPlay(ğ’«, i, N, Ï€i)
end

function update!(Ï€i::FictitiousPlay, a)
    N, ğ’«, â„, i = Ï€i.N, Ï€i.ğ’«, Ï€i.ğ’«.â„, Ï€i.i
    for (j, aj) in enumerate(a)
        N[j][aj] += 1
    end
    p(j) = SimpleGamePolicy(aj => u / sum(values(N[j])) for (aj, u) in N[j])
    Ï€ = [p(j) for j in â„]
    Ï€i.Ï€i = best_response(ğ’«, Ï€, i)
end
