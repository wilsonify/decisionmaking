struct ClampedSurrogateUpdate
    """
    an implementation of clamped surrogate policy
optimization, which returns a new
policy parameterization for policy
Ï€(s) of an MDP ð’« with initial state
distribution b . This implementation samples m trajectories to depth
d , and then uses them to estimate
the policy gradient in k_max subsequent updates. The policy gradient
using the clamped objective is constructed using the policy gradients
âˆ‡p with clamping parameter Ïµ .
    """
    ð’«::Any # problem
    b::Any # initial state distribution
    d::Any # depth
    m::Any # number of trajectories
    Ï€::Any # policy
    p::Any # policy likelihood
    âˆ‡Ï€::Any # policy likelihood gradient
    Ïµ::Any # divergence bound
    Î±::Any # step size
    k_max::Any # number of iterations per update
end
function clamped_gradient(M::ClampedSurrogateUpdate, Î¸, Î¸â€², Ï„s)
    d, p, âˆ‡Ï€, Ïµ, Î³ = M.d, M.p, M.âˆ‡Ï€, M.Ïµ, M.ð’«.Î³
    R(Ï„, j) = sum(r * Î³^(k - 1) for (k, (s, a, r)) in zip(j:d, Ï„[j:end]))
    âˆ‡f(a, s, r_togo) = begin
        P = p(Î¸, a, s)
        w = p(Î¸â€², a, s) / P
        if (r_togo > 0 && w > 1 + Ïµ) || (r_togo < 0 && w < 1 - Ïµ)
            return zeros(length(Î¸))
        end
        return âˆ‡Ï€(Î¸â€², a, s) * r_togo / P
    end
    âˆ‡f(Ï„) = mean(âˆ‡f(a, s, R(Ï„, k)) for (k, (s, a, r)) in enumerate(Ï„))
    return mean(âˆ‡f(Ï„) for Ï„ in Ï„s)
end
function update(M::ClampedSurrogateUpdate, Î¸)
    ð’«, b, d, m, Ï€, Î±, k_max = M.ð’«, M.b, M.d, M.m, M.Ï€, M.Î±, M.k_max
    Ï€Î¸(s) = Ï€(Î¸, s)
    Ï„s = [simulate(ð’«, rand(b), Ï€Î¸, d) for i = 1:m]
    Î¸â€² = copy(Î¸)
    for k = 1:k_max
        Î¸â€² += Î± * clamped_gradient(M, Î¸, Î¸â€², Ï„s)
    end
    return Î¸â€²
end
