"""
Monte Carlo policy evaluation of a policy Ï€ . The
method runs m rollouts to depth d
according to the dynamics specified by the problem ð’« . Each rollout
is run from an initial state sampled
from state distribution b . The final
line in this algorithm block evaluates a policy Ï€ parameterized by
Î¸ , which will be useful in the algorithms in this chapter that attempt
to find a Î¸ that maximizes U .
"""
struct MonteCarloPolicyEvaluation
    ð’«::Any # problem
    b::Any # initial state distribution
    d::Any # depth
    m::Any # number of samples
end
function (U::MonteCarloPolicyEvaluation)(Ï€)
    R(Ï€) = rollout(U.ð’«, rand(U.b), Ï€, U.d)
    return mean(R(Ï€) for i = 1:U.m)
end
(U::MonteCarloPolicyEvaluation)(Ï€, Î¸) = U(s -> Ï€(Î¸, s))


struct HookeJeevesPolicySearch
    """
    Policy search
using the Hooke-Jeeves method,
which returns a Î¸ that has been optimized with respect to U . The policy Ï€ takes as input a parameter Î¸
and state s . This implementation
starts with an initial Î¸ . The step size
Î± is reduced by a factor of c if no
neighbor improves the objective. Iterations are run until the step size
is less than Ïµ .
    """
    Î¸::Any # initial parameterization
    Î±::Any # step size
    c::Any # step size reduction factor
    Ïµ::Any # termination step size
end
function optimize(M::HookeJeevesPolicySearch, Ï€, U)
    Î¸, Î¸â€², Î±, c, Ïµ = copy(M.Î¸), similar(M.Î¸), M.Î±, M.c, M.Ïµ
    u, n = U(Ï€, Î¸), length(Î¸)
    while Î± > Ïµ
        copyto!(Î¸â€², Î¸)
        best = (i = 0, sgn = 0, u = u)
        for i = 1:n
            for sgn in (-1, 1)
                Î¸â€²[i] = Î¸[i] + sgn * Î±
                uâ€² = U(Ï€, Î¸â€²)
                if uâ€² > best.u
                    best = (i = i, sgn = sgn, u = uâ€²)
                end
            end
            Î¸â€²[i] = Î¸[i]
        end
        if best.i != 0
            Î¸[best.i] += best.sgn * Î±
            u = best.u
        else
            Î± *= c
        end
    end
    return Î¸
end

struct GeneticPolicySearch
    """
    A genetic policy
search method for iteratively updating a population of policy parameterizations Î¸s , which takes a
policy evaluation function U , a policy Ï€(Î¸, s) , a perturbation standard deviation Ïƒ , an elite sample
count m_elite , and an iteration
count k_max . The best m_elite samples from each iteration are used to
generate the samples for the subsequent iteration.
    """
    # initial population
    Î¸s::Any
    Ïƒ::Any
    # initial standard devidation
    m_elite::Any # number of elite samples
    k_max::Any
    # number of iterations
end
function optimize(M::GeneticPolicySearch, Ï€, U)
    Î¸s, Ïƒ = M.Î¸s, M.Ïƒ
    n, m = length(first(Î¸s)), length(Î¸s)
    for k = 1:M.k_max
        us = [U(Ï€, Î¸) for Î¸ in Î¸s]
        sp = sortperm(us, rev = true)
        Î¸_best = Î¸s[sp[1]]
        rand_elite() = Î¸s[sp[rand(1:M.m_elite)]]
        Î¸s = [rand_elite() + Ïƒ .* randn(n) for i = 1:(m-1)]
        push!(Î¸s, Î¸_best)
    end
    return last(Î¸s)
end
