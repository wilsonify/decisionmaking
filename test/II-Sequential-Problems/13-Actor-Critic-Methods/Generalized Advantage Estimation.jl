struct GeneralizedAdvantageEstimation
    """
    Generalized advantage estimation for computing
both a policy gradient and a value
function gradient for an MDP ğ’«
with initial state distribution b . The
policy is parameterized by Î¸ and
has a log-gradient âˆ‡logÏ€ . The value
function U is parameterized by Ï•
and has a gradient âˆ‡U . This method
runs m rollouts to depth d . The generalized advantage is computed
with exponential weighting Î» using
equation (13.21) with a finite horizon. The implementation here is a
simplified version of what was presented in the original paper, which
included aspects of trust regions
when taking steps.
    """
    # problem
    ğ’«
    b
    # initial state distribution
    d
    # depth
    m
    # number of samples
    âˆ‡logÏ€ # gradient of log likelihood âˆ‡logÏ€(Î¸,a,s)
    U
    # parameterized value function U(Ï•, s)
    âˆ‡U
    # gradient of value function âˆ‡U(Ï•,s)
    Î»
    # weight âˆˆ [0,1]
    end
    function gradient(M::GeneralizedAdvantageEstimation, Ï€, Î¸, Ï•)
    ğ’«, b, d, m, âˆ‡logÏ€ = M.ğ’«, M.b, M.d, M.m, M.âˆ‡logÏ€
    U, âˆ‡U, Î³, Î» = M.U, M.âˆ‡U, M.ğ’«.Î³, M.Î»
    Ï€Î¸(s) = Ï€(Î¸, s)
    R(Ï„,j) = sum(r*Î³^(k-1) for (k,(s,a,r)) in enumerate(Ï„[j:end]))
    Î´(Ï„,j) = Ï„[j][3] + Î³*U(Ï•,Ï„[j+1][1]) - U(Ï•,Ï„[j][1])
    A(Ï„,j) = sum((Î³*Î»)^(â„“-1)*Î´(Ï„, j+â„“-1) for â„“ in 1:d-j)
    âˆ‡UÎ¸(Ï„) = sum(âˆ‡logÏ€(Î¸,a,s)*A(Ï„,j)*Î³^(j-1)
    for (j, (s,a,r)) in enumerate(Ï„[1:end-1]))
    âˆ‡â„“Ï•(Ï„) = sum((U(Ï•,s) - R(Ï„,j))*âˆ‡U(Ï•,s)
    for (j, (s,a,r)) in enumerate(Ï„))
    trajs = [simulate(ğ’«, rand(b), Ï€Î¸, d) for i in 1:m]
    return mean(âˆ‡UÎ¸(Ï„) for Ï„ in trajs), mean(âˆ‡â„“Ï•(Ï„) for Ï„ in trajs)
    end