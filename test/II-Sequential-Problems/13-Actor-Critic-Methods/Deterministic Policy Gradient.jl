struct DeterministicPolicyGradient
    """
    The deterministic
policy gradient method for computing a policy gradient âˆ‡Î¸ for a
deterministic policy Ï€ and a value
function gradient âˆ‡Ï• for a continuous action MDP ğ’« with initial state
distribution b . The policy is parameterized by Î¸ and has a gradient
âˆ‡Ï€ that produces a matrix where
each column is the gradient with
respect to that continuous action
component. The value function Q is
parameterized by Ï• and has a gradient âˆ‡QÏ• with respect to the parameterization and gradient âˆ‡Qa with
respect to the action. This method
runs m rollouts to depth d , and performs exploration using 0-mean
Gaussian noise with standard deviation Ïƒ .
    """
    # problem
    ğ’«
    b
    # initial state distribution
    d
    # depth
    m
    # number of samples
    âˆ‡Ï€
    # gradient of deterministic policy Ï€(Î¸, s)
    Q
    # parameterized value function Q(Ï•,s,a)
    âˆ‡QÏ•
    # gradient of value function with respect to Ï•
    âˆ‡Qa
    # gradient of value function with respect to a
    Ïƒ
    # policy noise
    end
    function gradient(M::DeterministicPolicyGradient, Ï€, Î¸, Ï•)
    ğ’«, b, d, m, âˆ‡Ï€ = M.ğ’«, M.b, M.d, M.m, M.âˆ‡Ï€
    Q, âˆ‡QÏ•, âˆ‡Qa, Ïƒ, Î³ = M.Q, M.âˆ‡QÏ•, M.âˆ‡Qa, M.Ïƒ, M.ğ’«.Î³
    Ï€_rand(s) = Ï€(Î¸, s) + Ïƒ*randn()*I
    âˆ‡UÎ¸(Ï„) = sum(âˆ‡Ï€(Î¸,s)*âˆ‡Qa(Ï•,s,Ï€(Î¸,s))*Î³^(j-1) for (j,(s,a,r))
    in enumerate(Ï„))
    âˆ‡â„“Ï•(Ï„,j) = begin
    s, a, r = Ï„[j]
    sâ€² = Ï„[j+1][1]
    aâ€² = Ï€(Î¸,sâ€²)
    Î´ = r + Î³*Q(Ï•,sâ€²,aâ€²) - Q(Ï•,s,a)
    return Î´*(Î³*âˆ‡QÏ•(Ï•,sâ€²,aâ€²) - âˆ‡QÏ•(Ï•,s,a))
    end
    âˆ‡â„“Ï•(Ï„) = sum(âˆ‡â„“Ï•(Ï„,j) for j in 1:length(Ï„)-1)
    trajs = [simulate(ğ’«, rand(b), Ï€_rand, d) for i in 1:m]
    return mean(âˆ‡UÎ¸(Ï„) for Ï„ in trajs), mean(âˆ‡â„“Ï•(Ï„) for Ï„ in trajs)
    end