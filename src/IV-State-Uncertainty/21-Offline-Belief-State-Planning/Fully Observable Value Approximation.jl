
"""
Iteration structure
for updating a set of alpha vectors Î“ used by several of the methods in this chapter. The various
methods, including QMDP, differ
in their implementation of update .
After k_max iterations, this function
returns a policy represented by the
alpha vectors in Î“ .
"""
function alphavector_iteration(problem::POMDP, M, Î“)
    for k = 1:M.k_max
        Î“ = update(problem, M, Î“)
    end
    return Î“
end


"""
 The QMDP algorithm, which finds an approximately optimal policy for an infinite horizon POMDP with a discrete state and action space, where
k_max is the number of iterations.
QMDP assumes perfect observability.
"""
struct QMDP
    k_max::Any # maximum number of iterations
end
function update(problem::POMDP, M::QMDP, Î“)
    ğ’®, ğ’œ, R, T, Î³ = problem.ğ’®, problem.ğ’œ, problem.R, problem.T, problem.Î³
    Î“â€² = [
        [
            R(s, a) +
            Î³ * sum(T(s, a, sâ€²) * maximum(Î±â€²[j] for Î±â€² in Î“) for (j, sâ€²) in enumerate(ğ’®))
            for s in ğ’®
        ] for a in ğ’œ
    ]
    return Î“â€²
end

function solve(M::QMDP, problem::POMDP)
    Î“ = [zeros(length(problem.ğ’®)) for a in problem.ğ’œ]
    Î“ = alphavector_iteration(problem, M, Î“)
    return AlphaVectorPolicy(problem, Î“, problem.ğ’œ)
end
