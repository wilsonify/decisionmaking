"""
Iteration structure
for updating a set of alpha vectors Î“ used by several of the methods in this chapter. The various
methods, including QMDP, differ
in their implementation of update .
After k_max iterations, this function
returns a policy represented by the
alpha vectors in Î“ .
"""
function alphavector_iteration(ğ’«::POMDP, M, Î“)
    for k = 1:M.k_max
        Î“ = update(ğ’«, M, Î“)
    end
    return Î“
end


"""
 The QMDP algorithm, which finds an approximately optimal policy for an infinite horizon POMDP with a discrete state and action space, where
k_max is the number of iterations.
QMDP assumes perfect observability.
"""
struct QMDP
    k_max::Any # maximum number of iterations
end
function update(ğ’«::POMDP, M::QMDP, Î“)
    ğ’®, ğ’œ, R, T, Î³ = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.R, ğ’«.T, ğ’«.Î³
    Î“â€² = [
        [
            R(s, a) +
            Î³ * sum(T(s, a, sâ€²) * maximum(Î±â€²[j] for Î±â€² in Î“) for (j, sâ€²) in enumerate(ğ’®))
            for s in ğ’®
        ] for a in ğ’œ
    ]
    return Î“â€²
end

function solve(M::QMDP, ğ’«::POMDP)
    Î“ = [zeros(length(ğ’«.ğ’®)) for a in ğ’«.ğ’œ]
    Î“ = alphavector_iteration(ğ’«, M, Î“)
    return AlphaVectorPolicy(ğ’«, Î“, ğ’«.ğ’œ)
end
