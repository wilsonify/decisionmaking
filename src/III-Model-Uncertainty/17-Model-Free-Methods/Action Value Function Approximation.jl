"""
    The Q-learning
    update with action value function
    approximation. With each new ex    perience tuple s , a , r , sâ€² , we up    date our vector Î¸ with constant
    learning rate Î± . Our parameter    ized action value function is given
    by Q(Î¸,s,a) and its gradient is
    âˆ‡Q(Î¸,s,a) .
    """
struct GradientQLearning
    ğ’œ::Any # action space (assumes 1:nactions)
    Î³::Any # discount
    Q::Any # parameterized action value function Q(Î¸,s,a)
    âˆ‡Q::Any # gradient of action value function
    Î¸::Any # action value function parameter
    Î±::Any # learning rate
end

function lookahead(model::GradientQLearning, s, a)
    return model.Q(model.Î¸, s, a)
end

function update!(model::GradientQLearning, s, a, r, sâ€²)
    ğ’œ, Î³, Q, Î¸, Î± = model.ğ’œ, model.Î³, model.Q, model.Î¸, model.Î±
    u = maximum(Q(Î¸, sâ€², aâ€²) for aâ€² in ğ’œ)
    Î” = (r + Î³ * u - Q(Î¸, s, a)) * model.âˆ‡Q(Î¸, s, a)
    Î¸[:] += Î± * scale_gradient(Î”, 1)
    return model
end
