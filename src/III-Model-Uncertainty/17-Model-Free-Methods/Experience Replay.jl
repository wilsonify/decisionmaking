"""
    Q-learning with
    function approximation and ex    perience replay. The update de    pends on a parameterized policy
    Q(Î¸,s,a) and gradient âˆ‡Q(Î¸,s,a) .
    It updates the parameter vector Î¸
    and the circular memory buffer
    provided by DataStructures.jl .
    It updates Î¸ every m steps using
    a gradient estimated from m_grad
    samples from the buffer.
"""
struct ReplayGradientQLearning

    ğ’œ::Any # action space (assumes 1:nactions)
    Î³::Any     # discount
    Q::Any     # parameterized action value funciton Q(Î¸,s,a)
    âˆ‡Q::Any     # gradient of action value function
    Î¸::Any     # action value function parameter
    Î±::Any     # learning rate
    buffer::Any # circular memory buffer
    m::Any     # number of steps between gradient updates
    m_grad::Any # batch size
end

function lookahead(model::ReplayGradientQLearning, s, a)
    return model.Q(model.Î¸, s, a)
end
function update!(model::ReplayGradientQLearning, s, a, r, sâ€²)
    ğ’œ, Î³, Q, Î¸, Î± = model.ğ’œ, model.Î³, model.Q, model.Î¸, model.Î±
    buffer, m, m_grad = model.buffer, model.m, model.m_grad
    if isfull(buffer)
        U(s) = maximum(Q(Î¸, s, a) for a in ğ’œ)
        âˆ‡Q(s, a, r, sâ€²) = (r + Î³ * U(sâ€²) - Q(Î¸, s, a)) * model.âˆ‡Q(Î¸, s, a)
        Î” = mean(âˆ‡Q(s, a, r, sâ€²) for (s, a, r, sâ€²) in rand(buffer, m_grad))
        Î¸[:] += Î± * scale_gradient(Î”, 1)
        for i = 1:m # discard oldest experiences
            popfirst!(buffer)
        end
    else
        push!(buffer, (s, a, r, sâ€²))
    end
    return model
end
