"""
The Q-learning
update for model-free reinforcement learning, which can be applied to problems with unknown
transition and reward functions.
The update modifies Q , which is a
matrix of state-action values. This
update function can be used together with an exploration strategy,
such as Ç«-greedy in the simulate
function in algorithm 15.9.
"""
mutable struct QLearning
    ğ’®::Any #     state space (assumes 1:nstates)
    ğ’œ::Any #     action space (assumes 1:nactions)
    Î³::Any #     discount
    Q::Any #     action value function
    Î±::Any #     learning rate
end


lookahead(model::QLearning, s, a) = model.Q[s, a]
function update!(model::QLearning, s, a, r, sâ€²)
    Î³, Q, Î± = model.Î³, model.Q, model.Î±
    Q[s, a] += Î± * (r + Î³ * maximum(Q[sâ€², :]) - Q[s, a])
    return model
end


"""
An example of how
to use an exploration strategy with
Q-learning in simulation. The parameter settings are notional.

Suppose we want to apply Q-learning to an MDP problem ğ’« . We can construct an exploration policy, such as the Ç«-greedy policy implemented in
algorithm 16.6 from the previous chapter. The Q-learning model comes from
algorithm 17.2 and the simulate function is implemented in algorithm 15.9.
"""

Q = zeros(length(ğ’«.ğ’®), length(ğ’«.ğ’œ))
Î± = 0.2 # learning rate
model = QLearning(ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.Î³, Q, Î±)
Ïµ = 0.1 # probability of random action
Î± = 1.0 # exploration decay factor
Ï€ = EpsilonGreedyExploration(Ïµ, Î±)
k = 20 # number of steps to simulate
s = 1 # initial state
simulate(ğ’«, model, Ï€, k, s)