"""
    The prioritized
    sweeping algorithm maintains a
    priority queue pq of states that determines which are to be updated.
    With each update, we set the previous state to have infinite priority. We then perform m Bellman updates of the value function U at the
    highest priority states.
"""
struct PrioritizedUpdate
    m::Any # number of updates
    pq::Any # priority queue
end
function update!(planner::PrioritizedUpdate, model, s)
    N, U, pq = model.N, model.U, planner.pq
    ğ’®, ğ’œ = model.ğ’®, model.ğ’œ
    u = U[s]
    U[s] = backup(model, U, s)
    for sâ» in ğ’®
        for aâ» in ğ’œ
            n_sa = sum(N[sâ», aâ», sâ€²] for sâ€² in ğ’®)
            if n_sa > 0
                T = N[sâ», aâ», s] / n_sa
                priority = T * abs(U[s] - u)
                pq[sâ»] = max(get(pq, sâ», -Inf), priority)
            end
        end
    end
    return planner
end


function update!(planner::PrioritizedUpdate, model, s, a, r, sâ€²)
    planner.pq[s] = Inf
    for i = 1:planner.m
        if isempty(planner.pq)
            break
        end
        update!(planner, model, dequeue!(planner.pq))
    end
    return planner
end

"""
The Ç«-greedy
exploration strategy for maximum
likelihood model estimates. It
chooses a random action with
probability Ïµ , otherwise it uses the
model to extract the greedy action.
"""
function (Ï€::EpsilonGreedyExploration)(model, s)
    ğ’œ, Ïµ = model.ğ’œ, Ï€.Ïµ
    if rand() < Ïµ
        return rand(ğ’œ)
    end
    Q(s, a) = lookahead(model, s, a)
    return argmax(a -> Q(s, a), ğ’œ)
end


mutable struct RmaxMDP
    """
    The R-MAX
    exploration strategy modifies the
    transition and reward model from
    maximum likelihood estimation.
    It assigns maximum reward
    rmax to any underexplored
    state-action pair, defined as being
    those that have been tried fewer
    than m times. In addition, all
    underexplored state-action pairs
    are modeled as transitioning
    to the same state. This RmaxMDP
    can be used as a replacement
    for the MaximumLikelihoodMDP
    introduced in algorithm 16.1.
    """
    ğ’®::Any # state space (assumes 1:nstates)
    ğ’œ::Any # action space (assumes 1:nactions)
    N::Any # transition count N(s,a,sâ€²)
    Ï::Any # reward sum Ï(s, a)
    Î³::Any # discount
    U::Any # value function
    planner::Any
    # count threshold
    m::Any
    rmax::Any # maximum reward
end
function lookahead(model::RmaxMDP, s, a)
    ğ’®, U, Î³ = model.ğ’®, model.U, model.Î³
    n = sum(model.N[s, a, :])
    if n < model.m
        return model.rmax / (1 - Î³)
    end
    r = model.Ï[s, a] / n
    T(s, a, sâ€²) = model.N[s, a, sâ€²] / n
    return r + Î³ * sum(T(s, a, sâ€²) * U[sâ€²] for sâ€² in ğ’®)
end

function backup(model::RmaxMDP, U, s)
    return maximum(lookahead(model, s, a) for a in model.ğ’œ)
end
function update!(model::RmaxMDP, s, a, r, sâ€²)
    model.N[s, a, sâ€²] += 1
    model.Ï[s, a] += r
    update!(model.planner, model, s, a, r, sâ€²)
    return model
end
function MDP(model::RmaxMDP)
    N, Ï, ğ’®, ğ’œ, Î³ = model.N, model.Ï, model.ğ’®, model.ğ’œ, model.Î³
    T, R, m, rmax = similar(N), similar(Ï), model.m, model.rmax
    for s in ğ’®
        for a in ğ’œ
            n = sum(N[s, a, :])
            if n < m
                T[s, a, :] .= 0.0
                T[s, a, s] = 1.0
                R[s, a] = rmax
            else
                T[s, a, :] = N[s, a, :] / n
                R[s, a] = Ï[s, a] / n
            end
        end
    end
    return MDP(T, R, Î³)
end
