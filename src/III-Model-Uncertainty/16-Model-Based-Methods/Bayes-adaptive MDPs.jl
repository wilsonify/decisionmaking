mutable struct BayesianMDP
    """
    A Bayesian update method when the posterior
    distribution over transition models is represented as a product of
    Dirichlet distributions. We assume
    in this implementation that the reward model R is known, though
    we can use Bayesian methods to
    estimate expected reward from experience. The matrix D associates
    Dirichlet distributions with every
    state-action pair to model uncertainty in the transition to their successor states.
    """
    ğ’®::Any # state space (assumes 1:nstates)
    ğ’œ::Any # action space (assumes 1:nactions)
    D::Any # Dirichlet distributions D[s,a]
    R::Any # reward function as matrix (not estimated)
    Î³::Any # discount
    U::Any # value function
    planner::Any
end
function lookahead(model::BayesianMDP, s, a)
    ğ’®, U, Î³ = model.ğ’®, model.U, model.Î³
    n = sum(model.D[s, a].alpha)
    if n == 0
        return 0.0
    end
    r = model.R(s, a)
    T(s, a, sâ€²) = model.D[s, a].alpha[sâ€²] / n
    return r + Î³ * sum(T(s, a, sâ€²) * U[sâ€²] for sâ€² in ğ’®)
end


function update!(model::BayesianMDP, s, a, r, sâ€²)
    Î± = model.D[s, a].alpha
    Î±[sâ€²] += 1
    model.D[s, a] = Dirichlet(Î±)
    update!(model.planner, model, s, a, r, sâ€²)
    return model
end
