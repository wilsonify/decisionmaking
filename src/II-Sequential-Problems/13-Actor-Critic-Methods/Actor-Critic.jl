struct ActorCritic
    """
    A basic actor-critic
method for computing both a policy gradient and a value function
gradient for an MDP ğ’« with initial
state distribution b . The policy Ï€
is parameterized by Î¸ and has a
log-gradient âˆ‡logÏ€ . The value function U is parameterized by Ï• and the
gradient of its objective function is
âˆ‡U . This method runs m rollouts to
depth d . The results are used to update Î¸ and Ï• . The policy parameterization is updated in the direction of âˆ‡Î¸ to maximize the expected
value, whereas the value function
parameterization is updated in the
negative direction of âˆ‡Ï• to minimize the value loss.
    """
    # problem
    ğ’«
    b
    # initial state distribution
    d
    # depth
    m
    # number of samples
    âˆ‡logÏ€ # gradient of log likelihood âˆ‡logÏ€(Î¸,a,s)
    U
    # parameterized value function U(Ï•, s)
    âˆ‡U
    # gradient of value function âˆ‡U(Ï•,s)
    end
    function gradient(M::ActorCritic, Ï€, Î¸, Ï•)
    ğ’«, b, d, m, âˆ‡logÏ€ = M.ğ’«, M.b, M.d, M.m, M.âˆ‡logÏ€
    U, âˆ‡U, Î³ = M.U, M.âˆ‡U, M.ğ’«.Î³
    Ï€Î¸(s) = Ï€(Î¸, s)
    R(Ï„,j) = sum(r*Î³^(k-1) for (k,(s,a,r)) in enumerate(Ï„[j:end]))
    A(Ï„,j) = Ï„[j][3] + Î³*U(Ï•,Ï„[j+1][1]) - U(Ï•,Ï„[j][1])
    âˆ‡UÎ¸(Ï„) = sum(âˆ‡logÏ€(Î¸,a,s)*A(Ï„,j)*Î³^(j-1) for (j, (s,a,r))
    in enumerate(Ï„[1:end-1]))
    âˆ‡â„“Ï•(Ï„) = sum((U(Ï•,s) - R(Ï„,j))*âˆ‡U(Ï•,s) for (j, (s,a,r))
    in enumerate(Ï„))
    trajs = [simulate(ğ’«, rand(b), Ï€Î¸, d) for i in 1:m]
    return mean(âˆ‡UÎ¸(Ï„) for Ï„ in trajs), mean(âˆ‡â„“Ï•(Ï„) for Ï„ in trajs)
    end