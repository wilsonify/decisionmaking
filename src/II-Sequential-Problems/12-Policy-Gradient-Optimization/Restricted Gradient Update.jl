struct RestrictedPolicyUpdate
    """
    The update function for the restricted policy gradient method at Î¸ for a problem
ğ’« with initial state distribution b .
The gradient is estimated from an
initial state distribution b to depth
d with m simulations of parameterized policy Ï€(Î¸, s) with log policy
gradient âˆ‡logÏ€ .
    """

    ğ’«::Any # problem
    b::Any # initial state distribution
    d::Any # depth
    m::Any # number of samples
    âˆ‡logÏ€::Any # gradient of log likelihood
    Ï€::Any # policy
    Ïµ::Any # divergence bound
end
function update(M::RestrictedPolicyUpdate, Î¸)
    ğ’«, b, d, m, âˆ‡logÏ€, Ï€, Î³ = M.ğ’«, M.b, M.d, M.m, M.âˆ‡logÏ€, M.Ï€, M.ğ’«.Î³
    Ï€Î¸(s) = Ï€(Î¸, s)
    R(Ï„) = sum(r * Î³^(k - 1) for (k, (s, a, r)) in enumerate(Ï„))
    Ï„s = [simulate(ğ’«, rand(b), Ï€Î¸, d) for i = 1:m]
    âˆ‡log(Ï„) = sum(âˆ‡logÏ€(Î¸, a, s) for (s, a) in Ï„)
    âˆ‡U(Ï„) = âˆ‡log(Ï„) * R(Ï„)
    u = mean(âˆ‡U(Ï„) for Ï„ in Ï„s)
    return Î¸ + u * sqrt(2 * M.Ïµ / dot(u, u))
end
