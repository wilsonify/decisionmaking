struct NaturalPolicyUpdate
    """
    The update func-
    tion for the natural policy gradi-
    ent given policy Ï€(Î¸, s) for an
    MDP ğ’« with initial state distribu-
    tion b . The natural gradient with
    respect to the parameter vector Î¸ is
    estimated from m rollouts to depth
    d using the log policy gradients
    âˆ‡logÏ€ . The natural_update helper
    method conducts an update ac-
    cording to equation (12.12) given
    an objective gradient âˆ‡f(Ï„) and a
    Fisher matrix F(Ï„) for a list of tra-
    jectories.
    """
    ğ’«::Any # problem
    b::Any # initial state distribution
    d::Any # depth
    m::Any # number of samples
    âˆ‡logÏ€::Any # gradient of log likelihood
    Ï€::Any # policy
    Ïµ::Any # divergence bound
end

function natural_update(Î¸, âˆ‡f, F, Ïµ, Ï„s)
    âˆ‡fÎ¸ = mean(âˆ‡f(Ï„) for Ï„ in Ï„s)
    u = mean(F(Ï„) for Ï„ in Ï„s) \ âˆ‡fÎ¸
    return Î¸ + u * sqrt(2Ïµ / dot(âˆ‡fÎ¸, u))
end


function update(M::NaturalPolicyUpdate, Î¸)
    ğ’«, b, d, m, âˆ‡logÏ€, Ï€, Î³ = M.ğ’«, M.b, M.d, M.m, M.âˆ‡logÏ€, M.Ï€, M.ğ’«.Î³
    Ï€Î¸(s) = Ï€(Î¸, s)
    R(Ï„) = sum(r * Î³^(k - 1) for (k, (s, a, r)) in enumerate(Ï„))
    âˆ‡log(Ï„) = sum(âˆ‡logÏ€(Î¸, a, s) for (s, a) in Ï„)
    âˆ‡U(Ï„) = âˆ‡log(Ï„) * R(Ï„)
    F(Ï„) = âˆ‡log(Ï„) * âˆ‡log(Ï„)'
    Ï„s = [simulate(ğ’«, rand(b), Ï€Î¸, d) for i = 1:m]
    return natural_update(Î¸, âˆ‡U, F, M.Ïµ, Ï„s)
end
