struct MonteCarloTreeSearch
    """
    The Monte Carlo
tree search policy for finding of
an approximately optimal action
from a current state s . This twoargument version of argmax is defined in appendix G.5.
    """
    ğ’«::Any # problem
    N::Any # visit counts
    Q::Any # action value estimates
    d::Any # depth
    m::Any # number of simulations
    c::Any # exploration constant
    U::Any # value function estimate
end
function (Ï€::MonteCarloTreeSearch)(s)
    for k = 1:Ï€.m
        simulate!(Ï€, s)
    end
    return argmax(a -> Ï€.Q[(s, a)], Ï€.ğ’«.ğ’œ)
end

function simulate!(Ï€::MonteCarloTreeSearch, s, d = Ï€.d)
    if d â‰¤ 0
        return Ï€.U(s)
    end
    ğ’«, N, Q, c = Ï€.ğ’«, Ï€.N, Ï€.Q, Ï€.c
    ğ’œ, TR, Î³ = ğ’«.ğ’œ, ğ’«.TR, ğ’«.Î³
    if !haskey(N, (s, first(ğ’œ)))
        for a in ğ’œ
            N[(s, a)] = 0
            Q[(s, a)] = 0.0
        end
        return Ï€.U(s)
    end
    a = explore(Ï€, s)
    sâ€², r = TR(s, a)
    q = r + Î³ * simulate!(Ï€, sâ€², d - 1)
    N[(s, a)] += 1
    Q[(s, a)] += (q - Q[(s, a)]) / N[(s, a)]
    return q
end


"""
An exploration policy used in Monte Carlo tree search
when determining which nodes to
traverse through the search tree.
The policy is determined by a dictionary of state-action visitation
counts N and values Q , as well as
an exploration parameter c . When
N[(s,a)] = 0 , the policy returns
infinity.
"""
bonus(Nsa, Ns) = Nsa == 0 ? Inf : sqrt(log(Ns) / Nsa)

function explore(Ï€::MonteCarloTreeSearch, s)
    ğ’œ, N, Q, c = Ï€.ğ’«.ğ’œ, Ï€.N, Ï€.Q, Ï€.c
    Ns = sum(N[(s, a)] for a in ğ’œ)
    return argmax(a -> Q[(s, a)] + c * bonus(N[(s, a)], Ns), ğ’œ)
end
