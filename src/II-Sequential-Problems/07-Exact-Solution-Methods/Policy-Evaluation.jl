function lookahead(ğ’«::MDP, U, s, a)
    """
    Functions for com        puting the lookahead state-action
        value from a state s given an action
        a using an estimate of the value
        function U for the MDP ğ’« . The sec        ond version handles the case when
        U is a vector.
    """
    ğ’®, T, R, Î³ = ğ’«.ğ’®, ğ’«.T, ğ’«.R, ğ’«.Î³
    return R(s, a) + Î³ * sum(T(s, a, sâ€²) * U(sâ€²) for sâ€² in ğ’®)
end
function lookahead(ğ’«::MDP, U::Vector, s, a)
    ğ’®, T, R, Î³ = ğ’«.ğ’®, ğ’«.T, ğ’«.R, ğ’«.Î³
    return R(s, a) + Î³ * sum(T(s, a, sâ€²) * U[i] for (i, sâ€²) in enumerate(ğ’®))
end


function iterative_policy_evaluation(ğ’«::MDP, Ï€, k_max)
    """
    Iterative policy
evaluation, which iteratively computes the value function for a policy Ï€ for MDP ğ’« with discrete state
and action spaces using k_max iterations.
    """
    ğ’®, T, R, Î³ = ğ’«.ğ’®, ğ’«.T, ğ’«.R, ğ’«.Î³
    U = [0.0 for s in ğ’®]
    for k = 1:k_max
        U = [lookahead(ğ’«, U, s, Ï€(s)) for s in ğ’®]
    end
    return U
end


function policy_evaluation(ğ’«::MDP, Ï€)
    """
    Exact policy evaluation, which computes the value
function for a policy Ï€ for an MDP
ğ’« with discrete state and action
spaces.
    """
    ğ’®, R, T, Î³ = ğ’«.ğ’®, ğ’«.R, ğ’«.T, ğ’«.Î³
    Râ€² = [R(s, Ï€(s)) for s in ğ’®]
    Tâ€² = [T(s, Ï€(s), sâ€²) for s in ğ’®, sâ€² in ğ’®]
    return (I - Î³*Tâ€²)\Râ€²
    end