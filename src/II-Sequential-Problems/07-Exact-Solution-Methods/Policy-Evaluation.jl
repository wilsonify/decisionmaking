function lookahead(problem::MDP, U, s, a)
    """
    Functions for computing the lookahead state-action
        value from a state s given an action
        a using an estimate of the value
        function U for the MDP problem . The second version handles the case when
        U is a vector.
    """
    ğ’®, T, R, Î³ = problem.ğ’®, problem.T, problem.R, problem.Î³
    return R(s, a) + Î³ * sum(T(s, a, sâ€²) * U(sâ€²) for sâ€² in ğ’®)
end
function lookahead(problem::MDP, U::Vector, s, a)
    ğ’®, T, R, Î³ = problem.ğ’®, problem.T, problem.R, problem.Î³
    return R(s, a) + Î³ * sum(T(s, a, sâ€²) * U[i] for (i, sâ€²) in enumerate(ğ’®))
end


function iterative_policy_evaluation(problem::MDP, Ï€, k_max)
    """
    Iterative policy
evaluation, which iteratively computes the value function for a policy Ï€ for MDP problem with discrete state
and action spaces using k_max iterations.
    """
    ğ’®, T, R, Î³ = problem.ğ’®, problem.T, problem.R, problem.Î³
    U = [0.0 for s in ğ’®]
    for k = 1:k_max
        U = [lookahead(problem, U, s, Ï€(s)) for s in ğ’®]
    end
    return U
end


function policy_evaluation(problem::MDP, Ï€)
    """
    Exact policy evaluation, which computes the value
function for a policy Ï€ for an MDP
problem with discrete state and action
spaces.
    """
    ğ’®, R, T, Î³ = problem.ğ’®, problem.R, problem.T, problem.Î³
    Râ€² = [R(s, Ï€(s)) for s in ğ’®]
    Tâ€² = [T(s, Ï€(s), sâ€²) for s in ğ’®, sâ€² in ğ’®]
    return (I - Î³*Tâ€²)\Râ€²
    end