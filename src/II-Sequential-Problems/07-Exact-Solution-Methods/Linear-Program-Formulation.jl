using JuMP
"""
A method for
    solving a discrete MDP using a
    linear program formulation. For
    convenience in specifying the linear program, we define a function for converting an MDP into
    its tensor form, where the states
    and actions consist of integer indices, the reward function is a matrix, and the transition function is
    a three-dimensional tensor. It uses
    the JuMP.jl package for mathematical programming. The optimizer
    is set to use GLPK.jl , but others can
    be used instead. We also define the
    default solve behavior for MDPs to
    use this formulation.
"""
struct LinearProgramFormulation end

function tensorform(problem::MDP)
    ğ’®, ğ’œ, R, T = problem.ğ’®, problem.ğ’œ, problem.R, problem.T
    ğ’®â€² = eachindex(ğ’®)
    ğ’œâ€² = eachindex(ğ’œ)
    Râ€² = [R(s, a) for s in ğ’®, a in ğ’œ]
    Tâ€² = [T(s, a, sâ€²) for s in ğ’®, a in ğ’œ, sâ€² in ğ’®]
    return ğ’®â€², ğ’œâ€², Râ€², Tâ€²
end

solve(problem::MDP) = solve(LinearProgramFormulation(), problem)

function solve(M::LinearProgramFormulation, problem::MDP)
    ğ’®, ğ’œ, R, T = tensorform(problem)
    model = Model(GLPK.Optimizer)
    @variable(model, U[ğ’®])
    @objective(model, Min, sum(U))
    @constraint(model, [s = ğ’®, a = ğ’œ], U[s] â‰¥ R[s, a] + problem.Î³ * T[s, a, :] â‹… U)
    optimize!(model)
    return ValueFunctionPolicy(problem, value.(U))
end
