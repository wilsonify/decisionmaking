"""
A method for
    solving a discrete MDP using a
    linear program formulation. For
    convenience in specifying the lin    ear program, we define a func    tion for converting an MDP into
    its tensor form, where the states
    and actions consist of integer in    dices, the reward function is a ma    trix, and the transition function is
    a three-dimensional tensor. It uses
    the JuMP.jl package for mathemat    ical programming. The optimizer
    is set to use GLPK.jl , but others can
    be used instead. We also define the
    default solve behavior for MDPs to
    use this formulation.
"""
struct LinearProgramFormulation end
function tensorform(ğ’«::MDP)
    ğ’®, ğ’œ, R, T = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.R, ğ’«.T
    ğ’®â€² = eachindex(ğ’®)
    ğ’œâ€² = eachindex(ğ’œ)
    Râ€² = [R(s, a) for s in ğ’®, a in ğ’œ]
    Tâ€² = [T(s, a, sâ€²) for s in ğ’®, a in ğ’œ, sâ€² in ğ’®]
    return ğ’®â€², ğ’œâ€², Râ€², Tâ€²
end
solve(ğ’«::MDP) = solve(LinearProgramFormulation(), ğ’«)
function solve(M::LinearProgramFormulation, ğ’«::MDP)
    ğ’®, ğ’œ, R, T = tensorform(ğ’«)
    model = Model(GLPK.Optimizer)
    @variable(model, U[ğ’®])
    @objective(model, Min, sum(U))
    @constraint(model, [s = ğ’®, a = ğ’œ], U[s] â‰¥ R[s, a] + ğ’«.Î³ * T[s, a, :] â‹… U)
    optimize!(model)
    return ValueFunctionPolicy(ğ’«, value.(U))
end
