struct RewardToGoGradient
    """
    A method that
uses reward-to-go for estimating
a policy gradient of a policy Ï€(s)
for an MDP ğ’« with initial state dis-
tribution b . The gradient with re-
spect to the parameterization vec-
tor Î¸ is estimated from m rollouts to
depth d using the log policy gradi-
ents âˆ‡logÏ€ .
    """
    ğ’«::Any # problem
    b::Any # initial state distribution
    d::Any # depth
    m::Any # number of samples
    âˆ‡logÏ€::Any # gradient of log likelihood
end
function gradient(M::RewardToGoGradient, Ï€, Î¸)
    ğ’«, b, d, m, âˆ‡logÏ€, Î³ = M.ğ’«, M.b, M.d, M.m, M.âˆ‡logÏ€, M.ğ’«.Î³
    Ï€Î¸(s) = Ï€(Î¸, s)
    R(Ï„, j) = sum(r * Î³^(k - 1) for (k, (s, a, r)) in zip(j:d, Ï„[j:end]))
    âˆ‡U(Ï„) = sum(âˆ‡logÏ€(Î¸, a, s) * R(Ï„, j) for (j, (s, a, r)) in enumerate(Ï„))
    return mean(âˆ‡U(simulate(ğ’«, rand(b), Ï€Î¸, d)) for i = 1:m)
end

