function simulate(ð’«::MDP, s, Ï€, d)
    """
    A method for gen-
        erating a trajectory associated with
        problem ð’« starting in state s and
        executing policy Ï€ to depth d . It
        creates a vector Ï„ containing state-
        action-reward tuples.
    """
    Ï„ = []
    for i = 1:d
        a = Ï€(s)
        sâ€², r = ð’«.TR(s, a)
        push!(Ï„, (s, a, r))
        s = sâ€²
    end
    return Ï„
end

struct FiniteDifferenceGradient
    """
    A method for es-
        timating a policy gradient using
        finite differences for problem ð’« , a
        parameterized policy Ï€(Î¸, s) , and
        policy parameterization vector Î¸ .
        Utility estimates are made from m
        rollouts to depth d . The step size is
        given by Î´ .
    """
    ð’«::Any # problem
    b::Any # initial state distribution
    d::Any # depth
    m::Any # number of samples
    Î´::Any # step size
end

function gradient(M::FiniteDifferenceGradient, Ï€, Î¸)
    ð’«, b, d, m, Î´, Î³, n = M.ð’«, M.b, M.d, M.m, M.Î´, M.ð’«.Î³, length(Î¸)
    Î”Î¸(i) = [i == k ? Î´ : 0.0 for k in 1:n]
    R(Ï„) = sum(r*Î³^(k-1) for (k, (s,a,r)) in enumerate(Ï„))
    U(Î¸) = mean(R(simulate(ð’«, rand(b), s->Ï€(Î¸, s), d)) for i in 1:m)
    Î”U = [U(Î¸ + Î”Î¸(i)) - U(Î¸) for i in 1:n]
    return Î”U ./ Î´
    end