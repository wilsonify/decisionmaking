struct RegressionGradient
    """
    A method for estimating a policy gradient using
        finite differences for an MDP problem ,
        a stochastic parameterized policy
    π(θ, s) , and policy parameterization vector θ . Policy variation vectors are generated by normalizing
        normally-distributed samples and
        scaling by a perturbation scalar δ .
        A total of m parameter perturbations are generated, and each is
        evaluated in a rollout from an initial state drawn from b to depth d
        and compared to the original policy parameterization.
    """
    problem::Any # problem
    b::Any # initial state distribution
    d::Any # depth
    m::Any # number of samples
    δ::Any # step size
end
function gradient(M::RegressionGradient, π, θ)
    problem, b, d, m, δ, γ = M.problem, M.b, M.d, M.m, M.δ, M.problem.γ
    ΔΘ = [δ .* normalize(randn(length(θ)), 2) for i = 1:m]
    R(τ) = sum(r * γ^(k - 1) for (k, (s, a, r)) in enumerate(τ))
    U(θ) = R(simulate(problem, rand(b), s -> π(θ, s), d))
    ΔU = [U(θ + Δθ) - U(θ) for Δθ in ΔΘ]
    return pinv(reduce(hcat, ΔΘ)') * ΔU
end

f(x) = x^2 + 1e-2*randn()
m = 20
δ = 1e-2
ΔX = [δ.*randn() for i = 1:m]
x0 = 2.0
ΔF = [f(x0 + Δx) - f(x0) for Δx in ΔX]
pinv(ΔX) * ΔF