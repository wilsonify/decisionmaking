struct RegressionGradient
    """
    A method for estimating a policy gradient using
        finite differences for an MDP ð’« ,
        a stochastic parameterized policy
    Ï€(Î¸, s) , and policy parameterization vector Î¸ . Policy variation vectors are generated by normalizing
        normally-distributed samples and
        scaling by a perturbation scalar Î´ .
        A total of m parameter perturbations are generated, and each is
        evaluated in a rollout from an initial state drawn from b to depth d
        and compared to the original policy parameterization.
    """
    ð’«::Any # problem
    b::Any # initial state distribution
    d::Any # depth
    m::Any # number of samples
    Î´::Any # step size
end
function gradient(M::RegressionGradient, Ï€, Î¸)
    ð’«, b, d, m, Î´, Î³ = M.ð’«, M.b, M.d, M.m, M.Î´, M.ð’«.Î³
    Î”Î˜ = [Î´ .* normalize(randn(length(Î¸)), 2) for i = 1:m]
    R(Ï„) = sum(r * Î³^(k - 1) for (k, (s, a, r)) in enumerate(Ï„))
    U(Î¸) = R(simulate(ð’«, rand(b), s -> Ï€(Î¸, s), d))
    Î”U = [U(Î¸ + Î”Î¸) - U(Î¸) for Î”Î¸ in Î”Î˜]
    return pinv(reduce(hcat, Î”Î˜)') * Î”U
end

f(x) = x^2 + 1e-2*randn()
m = 20
Î´ = 1e-2
Î”X = [Î´.*randn() for i = 1:m]
x0 = 2.0
Î”F = [f(x0 + Î”x) - f(x0) for Î”x in Î”X]
pinv(Î”X) * Î”F