struct BaselineSubtractionGradient
    """
    Likelihood ratio
gradient estimation with reward-
to-go and baseline subtraction for
an MDP ğ’« , policy Ï€ , and initial state
distribution b . The gradient with
respect to the parameterization vec-
tor Î¸ is estimated from m rollouts to
depth d using the log policy gradi-
ents âˆ‡logÏ€ .
    """
    ğ’«::Any # problem
    b::Any # initial state distribution
    d::Any # depth
    m::Any # number of samples
    âˆ‡logÏ€::Any # gradient of log likelihood
end
function gradient(M::BaselineSubtractionGradient, Ï€, Î¸)
    ğ’«, b, d, m, âˆ‡logÏ€, Î³ = M.ğ’«, M.b, M.d, M.m, M.âˆ‡logÏ€, M.ğ’«.Î³
    Ï€Î¸(s) = Ï€(Î¸, s)
    â„“(a, s, k) = âˆ‡logÏ€(Î¸, a, s) * Î³^(k - 1)
    R(Ï„, k) = sum(r * Î³^(j - 1) for (j, (s, a, r)) in enumerate(Ï„[k:end]))
    numer(Ï„) = sum(â„“(a, s, k) .^ 2 * R(Ï„, k) for (k, (s, a, r)) in enumerate(Ï„))
    denom(Ï„) = sum(â„“(a, s, k) .^ 2 for (k, (s, a)) in enumerate(Ï„))
    base(Ï„) = numer(Ï„) ./ denom(Ï„)
    trajs = [simulate(ğ’«, rand(b), Ï€Î¸, d) for i = 1:m]
    rbase = mean(base(Ï„) for Ï„ in trajs)
    âˆ‡U(Ï„) = sum(â„“(a, s, k) .* (R(Ï„, k) .- rbase) for (k, (s, a, r)) in enumerate(Ï„))
    return mean(âˆ‡U(Ï„) for Ï„ in trajs)
end
