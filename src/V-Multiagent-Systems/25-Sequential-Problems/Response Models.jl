include("Markov Games.jl")
"""
For a Markov game ğ’«, 
we can compute a deterministic best response policy 
for agent i given that the other agents are playing policies in Ï€ . 
We can solve the MDP exactly using one of the methods from "Exact Solution Methods".
"""
function best_response(ğ’«::MG, Ï€, i)
    ğ’®, ğ’œ, R, T, Î³ = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.R, ğ’«.T, ğ’«.Î³
    Tâ€²(s, ai, sâ€²) = transition(ğ’«, s, joint(Ï€, SimpleGamePolicy(ai), i), sâ€²)
    Râ€²(s, ai) = reward(ğ’«, s, joint(Ï€, SimpleGamePolicy(ai), i), i)
    Ï€i = solve(MDP(Î³, ğ’®, ğ’œ[i], Tâ€², Râ€²))
    return MGPolicy(s => SimpleGamePolicy(Ï€i(s)) for s in ğ’®)
end

"""
The softmax response of agent i to joint policy Ï€
with precision parameter Î» .
"""
function softmax_response(ğ’«::MG, Ï€, i, Î»)
    ğ’®, ğ’œ, R, T, Î³ = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.R, ğ’«.T, ğ’«.Î³
    Tâ€²(s, ai, sâ€²) = transition(ğ’«, s, joint(Ï€, SimpleGamePolicy(ai), i), sâ€²)
    Râ€²(s, ai) = reward(ğ’«, s, joint(Ï€, SimpleGamePolicy(ai), i), i)
    mdp = MDP(Î³, ğ’®, joint(ğ’œ), Tâ€², Râ€²)
    Ï€i = solve(mdp)
    Q(s, a) = lookahead(mdp, Ï€i.U, s, a)
    p(s) = SimpleGamePolicy(a => exp(Î» * Q(s, a)) for a in ğ’œ[i])
    return MGPolicy(s => p(s) for s in ğ’®)
end
