

"""
Nash Q-learning 
for an agent i in an MG problem .
The algorithm performs joint-action Qlearning to learn a state-action
value function for all agents. 
A simple game is built with Q , and we compute a Nash equilibrium using algorithm 24.5. 
The equilibrium is then used to update the value function. 
This implementation also uses a variable learning rate proportional to the number of times
state-joint-action pairs are visited, which is stored in N . 
Additionally, it uses Ç«-greedy exploration to ensure all states and actions are explored.
"""
mutable struct NashQLearning
    problem::Any # Markov game
    i::Any # agent index
    Q::Any # state-action value estimates
    N::Any # history of actions performed
end





function NashQLearning(problem::MG, i)
    â„, ğ’®, ğ’œ = problem.â„, problem.ğ’®, problem.ğ’œ
    Q = Dict((j, s, a) => 0.0 for j in â„, s in ğ’®, a in joint(ğ’œ))
    N = Dict((s, a) => 1.0 for s in ğ’®, a in joint(ğ’œ))
    return NashQLearning(problem, i, Q, N)
end
function (Ï€i::NashQLearning)(s)
    problem, i, Q, N = Ï€i.problem, Ï€i.i, Ï€i.Q, Ï€i.N
    â„, ğ’®, ğ’œ, ğ’œi, Î³ = problem.â„, problem.ğ’®, problem.ğ’œ, problem.ğ’œ[Ï€i.i], problem.Î³
    M = NashEquilibrium()
    ğ’¢ = SimpleGame(Î³, â„, ğ’œ, a -> [Q[j, s, a] for j in â„])
    Ï€ = solve(M, ğ’¢)
    Ïµ = 1 / sum(N[s, a] for a in joint(ğ’œ))
    Ï€iâ€²(ai) = Ïµ / length(ğ’œi) + (1 - Ïµ) * Ï€[i](ai)
    return SimpleGamePolicy(ai => Ï€iâ€²(ai) for ai in ğ’œi)
end


function update!(Ï€i::NashQLearning, s, a, sâ€²)
    problem, â„, ğ’®, ğ’œ, R, Î³ = Ï€i.problem, Ï€i.problem.â„, Ï€i.problem.ğ’®, Ï€i.problem.ğ’œ, Ï€i.problem.R, Ï€i.problem.Î³
    i, Q, N = Ï€i.i, Ï€i.Q, Ï€i.N
    M = NashEquilibrium()
    ğ’¢ = SimpleGame(Î³, â„, ğ’œ, aâ€² -> [Q[j, sâ€², aâ€²] for j in â„])
    Ï€ = solve(M, ğ’¢)
    Ï€i.N[s, a] += 1
    Î± = 1 / sqrt(N[s, a])
    for j in â„
        Ï€i.Q[j, s, a] += Î± * (R(s, a)[j] + Î³ * utility(ğ’¢, Ï€, j) - Q[j, s, a])
    end
end
