"""
Nash Q-learning
for an agent i in an MG ğ’« . The algorithm performs joint-action Qlearning to learn a state-action
value function for all agents. A simple game is built with Q , and we
compute a Nash equilibrium using
algorithm 24.5. The equilibrium
is then used to update the value
function. This implementation also
uses a variable learning rate proportional to the number of times
state-joint-action pairs are visited,
which is stored in N . Additionally,
it uses Ç«-greedy exploration to ensure all states and actions are explored.
"""
mutable struct NashQLearning
    ğ’«::Any # Markov game
    i::Any # agent index
    Q::Any # state-action value estimates
    N::Any # history of actions performed
end





function NashQLearning(ğ’«::MG, i)
    â„, ğ’®, ğ’œ = ğ’«.â„, ğ’«.ğ’®, ğ’«.ğ’œ
    Q = Dict((j, s, a) => 0.0 for j in â„, s in ğ’®, a in joint(ğ’œ))
    N = Dict((s, a) => 1.0 for s in ğ’®, a in joint(ğ’œ))
    return NashQLearning(ğ’«, i, Q, N)
end
function (Ï€i::NashQLearning)(s)
    ğ’«, i, Q, N = Ï€i.ğ’«, Ï€i.i, Ï€i.Q, Ï€i.N
    â„, ğ’®, ğ’œ, ğ’œi, Î³ = ğ’«.â„, ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.ğ’œ[Ï€i.i], ğ’«.Î³
    M = NashEquilibrium()
    ğ’¢ = SimpleGame(Î³, â„, ğ’œ, a -> [Q[j, s, a] for j in â„])
    Ï€ = solve(M, ğ’¢)
    Ïµ = 1 / sum(N[s, a] for a in joint(ğ’œ))
    Ï€iâ€²(ai) = Ïµ / length(ğ’œi) + (1 - Ïµ) * Ï€[i](ai)
    return SimpleGamePolicy(ai => Ï€iâ€²(ai) for ai in ğ’œi)
end


function update!(Ï€i::NashQLearning, s, a, sâ€²)
    ğ’«, â„, ğ’®, ğ’œ, R, Î³ = Ï€i.ğ’«, Ï€i.ğ’«.â„, Ï€i.ğ’«.ğ’®, Ï€i.ğ’«.ğ’œ, Ï€i.ğ’«.R, Ï€i.ğ’«.Î³
    i, Q, N = Ï€i.i, Ï€i.Q, Ï€i.N
    M = NashEquilibrium()
    ğ’¢ = SimpleGame(Î³, â„, ğ’œ, aâ€² -> [Q[j, sâ€², aâ€²] for j in â„])
    Ï€ = solve(M, ğ’¢)
    Ï€i.N[s, a] += 1
    Î± = 1 / sqrt(N[s, a])
    for j in â„
        Ï€i.Q[j, s, a] += Î± * (R(s, a)[j] + Î³ * utility(ğ’¢, Ï€, j) - Q[j, s, a])
    end
end
