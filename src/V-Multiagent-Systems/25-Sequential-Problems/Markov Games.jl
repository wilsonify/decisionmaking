
struct MG
    """
    Data structure for a Markov game.
    """
    Î³::Any # discount factor
    â„::Any # agents
    ğ’®::Any # state space
    ğ’œ::Any # joint action space
    T::Any # transition function
    R::Any # joint reward function
end

struct MGPolicy
    """
    A Markov game
policy is a mapping from states to
simple game policies, introduced
in the previous chapter. We can
construct it by passing in a generator to construct the dictionary. The
probability a policy (either for a
Markov game or simple game) assigns to taking action ai from state
s is Ï€i (s,ai). Functions are also
provided for computing R i ( s, Ï€ ( s ))
and T ( s â€² | s, Ï€ ( s )) . The policy evaluation function will compute a vector representing U Ï€,i .
    """
    p::Any # dictionary mapping states to simple game policies
    MGPolicy(p::Base.Generator) = new(Dict(p))
end
(Ï€i::MGPolicy)(s, ai) = Ï€i.p[s](ai)
(Ï€i::SimpleGamePolicy)(s, ai) = Ï€i(ai)
probability(problem::MG, s, Ï€, a) = prod(Ï€j(s, aj) for (Ï€j, aj) in zip(Ï€, a))
reward(problem::MG, s, Ï€, i) = sum(problem.R(s, a)[i] * probability(problem, s, Ï€, a) for a in joint(problem.ğ’œ))
transition(problem::MG, s, Ï€, sâ€²) =
    sum(problem.T(s, a, sâ€²) * probability(problem, s, Ï€, a) for a in joint(problem.ğ’œ))
function policy_evaluation(problem::MG, Ï€, i)
    ğ’®, ğ’œ, R, T, Î³ = problem.ğ’®, problem.ğ’œ, problem.R, problem.T, problem.Î³
    p(s, a) = prod(Ï€j(s, aj) for (Ï€j, aj) in zip(Ï€, a))
    Râ€² = [sum(R(s, a)[i] * p(s, a) for a in joint(ğ’œ)) for s in ğ’®]
    Tâ€² = [sum(T(s, a, sâ€²) * p(s, a) for a in joint(ğ’œ)) for s in ğ’®, sâ€² in ğ’®]
    return (I - Î³ * Tâ€²) \ Râ€²
end







