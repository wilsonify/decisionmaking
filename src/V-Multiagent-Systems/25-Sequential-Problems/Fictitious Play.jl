
"""
This nonlinear
program computes a Nash equilibrium for a Markov game ğ’« .
"""
function tensorform(ğ’«::MG)
    â„, ğ’®, ğ’œ, R, T = ğ’«.â„, ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.R, ğ’«.T
    â„â€² = eachindex(â„)
    ğ’®â€² = eachindex(ğ’®)
    ğ’œâ€² = [eachindex(ğ’œ[i]) for i in â„]
    Râ€² = [R(s, a) for s in ğ’®, a in joint(ğ’œ)]
    Tâ€² = [T(s, a, sâ€²) for s in ğ’®, a in joint(ğ’œ), sâ€² in ğ’®]
    return â„â€², ğ’®â€², ğ’œâ€², Râ€², Tâ€²
end

function solve!(M::NashEquilibrium, ğ’«::MG)
    â„, ğ’®, ğ’œ, R, T = tensorform(ğ’«)
    ğ’®â€², ğ’œâ€², Î³ = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.Î³
    model = Model(Ipopt.Optimizer)
    @variable(model, U[â„, ğ’®])
    @variable(model, Ï€[i = â„, ğ’®, ai = ğ’œ[i]] â‰¥ 0)
    @NLobjective(
        model,
        Min,
        sum(
            U[i, s] - sum(
                prod(Ï€[j, s, a[j]] for j in â„) *
                (R[s, y][i] + Î³ * sum(T[s, y, sâ€²] * U[i, sâ€²] for sâ€² in ğ’®)) for
                (y, a) in enumerate(joint(ğ’œ))
            ) for i in â„, s in ğ’®
        )
    )
    @NLconstraint(
        model,
        [i = â„, s = ğ’®, ai = ğ’œ[i]],
        U[i, s] â‰¥ sum(
            prod(j == i ? (a[j] == ai ? 1.0 : 0.0) : Ï€[j, s, a[j]] for j in â„) *
            (R[s, y][i] + Î³ * sum(T[s, y, sâ€²] * U[i, sâ€²] for sâ€² in ğ’®)) for
            (y, a) in enumerate(joint(ğ’œ))
        )
    )
    @constraint(model, [i = â„, s = ğ’®], sum(Ï€[i, s, ai] for ai in ğ’œ[i]) == 1)
    optimize!(model)
    Ï€â€² = value.(Ï€)
    Ï€iâ€²(i, s) = SimpleGamePolicy(ğ’œâ€²[i][ai] => Ï€â€²[i, s, ai] for ai in ğ’œ[i])
    Ï€iâ€²(i) = MGPolicy(ğ’®â€²[s] => Ï€iâ€²(i, s) for s in ğ’®)
    return [Ï€iâ€²(i) for i in â„]
end

"""
Functions for taking a random step and running full
    simulations in Markov games. The
    simulate function will simulate the
    joint policy Ï€ for k_max steps starting from a state randomly sampled
    from b .
"""
function randstep(ğ’«::MG, s, a)
    sâ€² = rand(SetCategorical(ğ’«.ğ’®, [ğ’«.T(s, a, sâ€²) for sâ€² in ğ’«.ğ’®]))
    r = ğ’«.R(s, a)
    return sâ€², r
end
function simulate(ğ’«::MG, Ï€, k_max, b)
    s = rand(b)
    for k = 1:k_max
        a = Tuple(Ï€i(s)() for Ï€i in Ï€)
        sâ€², r = randstep(ğ’«, s, a)
        for Ï€i in Ï€
            update!(Ï€i, s, a, sâ€²)
        end
        s = sâ€²
    end
    return Ï€
end


mutable struct MGFictitiousPlay
    """
    Fictitious play for
        agent i in an MG ğ’« that maintains
        counts Ni of other agent action selections over time for each state
        and averages them assuming this is
        their stochastic policy. It then computes a best response to this policy and performs the corresponding utility-maximizing action.
    """
    ğ’«::Any # Markov game
    i::Any # agent index
    Qi::Any # state-action value estimates
    Ni::Any # state-action counts
end
function MGFictitiousPlay(ğ’«::MG, i)
    â„, ğ’®, ğ’œ, R = ğ’«.â„, ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.R
    Qi = Dict((s, a) => R(s, a)[i] for s in ğ’® for a in joint(ğ’œ))
    Ni = Dict((j, s, aj) => 1.0 for j in â„ for s in ğ’® for aj in ğ’œ[j])
    return MGFictitiousPlay(ğ’«, i, Qi, Ni)
end


function (Ï€i::MGFictitiousPlay)(s)
    ğ’«, i, Qi = Ï€i.ğ’«, Ï€i.i, Ï€i.Qi
    â„, ğ’®, ğ’œ, T, R, Î³ = ğ’«.â„, ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.T, ğ’«.R, ğ’«.Î³
    Ï€iâ€²(i, s) = SimpleGamePolicy(ai => Ï€i.Ni[i, s, ai] for ai in ğ’œ[i])
    Ï€iâ€²(i) = MGPolicy(s => Ï€iâ€²(i, s) for s in ğ’®)
    Ï€ = [Ï€iâ€²(i) for i in â„]
    U(s, Ï€) = sum(Ï€i.Qi[s, a] * probability(ğ’«, s, Ï€, a) for a in joint(ğ’œ))
    Q(s, Ï€) = reward(ğ’«, s, Ï€, i) + Î³ * sum(transition(ğ’«, s, Ï€, sâ€²) * U(sâ€², Ï€) for sâ€² in ğ’®)
    Q(ai) = Q(s, joint(Ï€, SimpleGamePolicy(ai), i))
    ai = argmax(Q, ğ’«.ğ’œ[Ï€i.i])
    return SimpleGamePolicy(ai)
end
function update!(Ï€i::MGFictitiousPlay, s, a, sâ€²)
    ğ’«, i, Qi = Ï€i.ğ’«, Ï€i.i, Ï€i.Qi
    â„, ğ’®, ğ’œ, T, R, Î³ = ğ’«.â„, ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.T, ğ’«.R, ğ’«.Î³
    for (j, aj) in enumerate(a)
        Ï€i.Ni[j, s, aj] += 1
    end
    Ï€iâ€²(i, s) = SimpleGamePolicy(ai => Ï€i.Ni[i, s, ai] for ai in ğ’œ[i])
    Ï€iâ€²(i) = MGPolicy(s => Ï€iâ€²(i, s) for s in ğ’®)
    Ï€ = [Ï€iâ€²(i) for i in â„]
    U(Ï€, s) = sum(Ï€i.Qi[s, a] * probability(ğ’«, s, Ï€, a) for a in joint(ğ’œ))
    Q(s, a) = R(s, a)[i] + Î³ * sum(T(s, a, sâ€²) * U(Ï€, sâ€²) for sâ€² in ğ’®)
    for a in joint(ğ’œ)
        Ï€i.Qi[s, a] = Q(s, a)
    end
end
