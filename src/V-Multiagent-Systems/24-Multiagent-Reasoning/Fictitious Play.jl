
"""
The simulation of a joint policy in simple game problem for k_max iterations. 
The joint policy Ï€ is a vector of policies that can be individually updated through calls to update!(Ï€i, a) .
"""
function simulate(problem::SimpleGame, Ï€, k_max)
    for k = 1:k_max
        a = [Ï€i() for Ï€i in Ï€]
        for Ï€i in Ï€
            update!(Ï€i, a)
        end
    end
    return Ï€
end


mutable struct FictitiousPlay
    """
    Fictitious play is
    a simple learning algorithm for an
    agent i of a simple game problem that
    maintains counts of other agent action selections over time and averages them assuming this is their
    stochastic policy. It then computes
    a best response to this policy and
    performs the corresponding utilitymaximizing action.
    (Ï€i::FictitiousPlay)() = Ï€i.Ï€i()
    (Ï€i::FictitiousPlay)(ai) = Ï€i.Ï€i(ai)
    """
    problem::Any # simple game
    i::Any # agent index
    N::Any # array of action count dictionaries
    Ï€i::Any # current policy
end
function FictitiousPlay(problem::SimpleGame, i)
    N = [Dict(aj => 1 for aj in problem.ğ’œ[j]) for j in problem.â„]
    Ï€i = SimpleGamePolicy(ai => 1.0 for ai in problem.ğ’œ[i])
    return FictitiousPlay(problem, i, N, Ï€i)
end

function update!(Ï€i::FictitiousPlay, a)
    N, problem, â„, i = Ï€i.N, Ï€i.problem, Ï€i.problem.â„, Ï€i.i
    for (j, aj) in enumerate(a)
        N[j][aj] += 1
    end
    p(j) = SimpleGamePolicy(aj => u / sum(values(N[j])) for (aj, u) in N[j])
    Ï€ = [p(j) for j in â„]
    Ï€i.Ï€i = best_response(problem, Ï€, i)
end
