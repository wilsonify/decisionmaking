
"""
An implementation of gradient ascent for an agent
i of a simple game problem . The algorithm
updates its distribution over actions incrementally following gradient ascent to improve the expected utility. The projection function from algorithm 23.6 is used
to ensure that the resulting policy
remains a valid probability distribution.
"""
mutable struct GradientAscent
    problem::Any # simple game
    i::Any # agent index
    t::Any # time step
    Ï€i::Any # current policy
end
function GradientAscent(problem::SimpleGame, i)
    uniform() = SimpleGamePolicy(ai => 1.0 for ai in problem.ğ’œ[i])
    return GradientAscent(problem, i, 1, uniform())
end
(Ï€i::GradientAscent)() = Ï€i.Ï€i()
(Ï€i::GradientAscent)(ai) = Ï€i.Ï€i(ai)
function update!(Ï€i::GradientAscent, a)
    problem, â„, ğ’œi, i, t = Ï€i.problem, Ï€i.problem.â„, Ï€i.problem.ğ’œ[Ï€i.i], Ï€i.i, Ï€i.t
    jointÏ€(ai) = [SimpleGamePolicy(j == i ? ai : a[j]) for j in â„]
    r = [utility(problem, jointÏ€(ai), i) for ai in ğ’œi]
    Ï€â€² = [Ï€i.Ï€i(ai) for ai in ğ’œi]
    Ï€ = project_to_simplex(Ï€â€² + r / sqrt(t))
    Ï€i.t = t + 1
    Ï€i.Ï€i = SimpleGamePolicy(ai => p for (ai, p) in zip(ğ’œi, Ï€))
end
